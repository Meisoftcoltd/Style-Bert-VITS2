# Style-Bert-VITS2

**Por favor, asegÃºrese de leer los [TÃ©rminos de uso y las Peticiones](/docs/TERMS_OF_USE.md) antes de usar.**

Bert-VITS2 con estilos de voz mÃ¡s controlables.

https://github.com/litagin02/Style-Bert-VITS2/assets/139731664/e853f9a2-db4a-4202-a1dd-56ded3c562a0

Puede instalarlo vÃ­a `pip install style-bert-vits2` (solo inferencia), vea [library.ipynb](/library.ipynb) para ejemplos de uso.

- **Video Tutorial** [YouTube](https://youtu.be/aTUSzgDl1iY)ã€€[NicoNico](https://www.nicovideo.jp/watch/sm43391524)
- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/litagin02/Style-Bert-VITS2/blob/master/colab.ipynb)
- [**Preguntas frecuentes** (FAQ)](/docs/FAQ.md)
- [ğŸ¤— Demo en lÃ­nea aquÃ­](https://huggingface.co/spaces/litagin/Style-Bert-VITS2-Editor-Demo)
- [ArtÃ­culo explicativo en Zenn](https://zenn.dev/litagin/articles/034819a5256ff4)

- [**PÃ¡gina de lanzamientos**](https://github.com/litagin02/Style-Bert-VITS2/releases/)ã€[Historial de actualizaciones](/docs/CHANGELOG.md)
  - 2025-08-24: Ver 2.7.0: Se aÃ±adiÃ³ GUI para conversiÃ³n a ONNX para integraciÃ³n con bibliotecas externas como [Aivis Project](https://aivis-project.com/), y se aÃ±adiÃ³ `litagin/anime-whisper` como modelo de reconocimiento de voz.
  - 2024-09-09: Ver 2.6.1: CorrecciÃ³n de errores en Google Colab, etc.
  - 2024-06-16: Ver 2.6.0 (AÃ±adida fusiÃ³n de diferencias de modelos, fusiÃ³n ponderada, fusiÃ³n de modelos nulos. Vea [este artÃ­culo](https://zenn.dev/litagin/articles/1297b1dc7bdc79) para usos).
  - 2024-06-14: Ver 2.5.1 (Cambio de tÃ©rminos de uso a peticiones).
  - 2024-06-02: Ver 2.5.0 (**[AÃ±adidos TÃ©rminos de Uso](/docs/TERMS_OF_USE.md)**, generaciÃ³n de estilos desde carpetas, adiciÃ³n de modelos Koharu Ami y Amitaro, instalaciÃ³n mÃ¡s rÃ¡pida, etc.).
  - 2024-03-16: ver 2.4.1 (**Cambio en el mÃ©todo de instalaciÃ³n mediante archivos bat**).
  - 2024-03-15: ver 2.4.0 (RefactorizaciÃ³n a gran escala y varias mejoras, conversiÃ³n a librerÃ­a).
  - 2024-02-26: ver 2.3 (Funciones de diccionario y editor).
  - 2024-02-09: ver 2.2
  - 2024-02-07: ver 2.1
  - 2024-02-03: ver 2.0 (JP-Extra)
  - 2024-01-09: ver 1.3
  - 2023-12-31: ver 1.2
  - 2023-12-29: ver 1.1
  - 2023-12-27: ver 1.0

Este repositorio se basa en [Bert-VITS2](https://github.com/fishaudio/Bert-VITS2) v2.1 y Japanese-Extra, Â¡muchas gracias al autor original!

**Resumen**

- Basado en Bert-VITS2 v2.1 y Japanese-Extra, que genera voz expresiva basada en el contenido del texto de entrada, permitiendo controlar libremente la emociÃ³n y el estilo de habla con intensidad.
- Incluso si no tiene Git o Python (para usuarios de Windows), puede instalar y entrenar fÃ¡cilmente (tomado en gran parte de [EasyBertVits2](https://github.com/Zuntan03/EasyBertVits2/)). TambiÃ©n soporta entrenamiento en Google Colab.
- Si solo lo usa para sÃ­ntesis de voz, funciona en CPU sin tarjeta grÃ¡fica.
- Para sÃ­ntesis de voz, se puede instalar como librerÃ­a Python con `pip install style-bert-vits2`. Vea [library.ipynb](/library.ipynb) para ejemplos.
- Incluye un servidor API que se puede usar para integraciÃ³n con otras herramientas (PR por [@darai0512](https://github.com/darai0512), gracias).
- La fortaleza de Bert-VITS2 es "leer textos alegres con alegrÃ­a y textos tristes con tristeza", por lo que puede generar voz expresiva incluso con el estilo predeterminado.


## CÃ³mo usar

- Para uso en CLI, consulte [aquÃ­](/docs/CLI.md).
- Consulte tambiÃ©n las [Preguntas frecuentes](/docs/FAQ.md).

### Entorno de ejecuciÃ³n

Se ha confirmado el funcionamiento de cada UI y API Server en SÃ­mbolo del sistema de Windows, WSL2 y Linux (Ubuntu Desktop). Si no tiene una GPU NVidia, no puede entrenar, pero puede realizar sÃ­ntesis de voz y fusiÃ³n.

### InstalaciÃ³n

Consulte [library.ipynb](/library.ipynb) para la instalaciÃ³n y uso como librerÃ­a Python con pip.

#### Para quienes no estÃ¡n familiarizados con Git o Python

Se asume Windows.

1. Descargue [este archivo zip](https://github.com/litagin02/Style-Bert-VITS2/releases/latest/download/sbv2.zip) y extrÃ¡igalo en una ubicaciÃ³n **sin espacios ni caracteres japoneses (o especiales) en la ruta**.
  - Si tiene tarjeta grÃ¡fica, haga doble clic en `Install-Style-Bert-VITS2.bat`.
  - Si no tiene tarjeta grÃ¡fica, haga doble clic en `Install-Style-Bert-VITS2-CPU.bat`. La versiÃ³n CPU no permite entrenamiento, solo sÃ­ntesis y fusiÃ³n.
2. Espere a que se instale el entorno necesario automÃ¡ticamente.
3. Si el editor de sÃ­ntesis de voz se inicia automÃ¡ticamente, la instalaciÃ³n fue exitosa. Los modelos predeterminados se descargan, asÃ­ que puede jugar con ellos de inmediato.

Si desea actualizar, haga doble clic en `Update-Style-Bert-VITS2.bat`.

Sin embargo, si actualiza desde una versiÃ³n anterior a **2.4.1** (2024-03-16), debe eliminar todo e instalar de nuevo. Disculpe las molestias. Consulte [CHANGELOG.md](/docs/CHANGELOG.md) para la migraciÃ³n.

#### Para quienes saben usar Git y Python

Se recomienda usar [uv](https://github.com/astral-sh/uv), una herramienta de gestiÃ³n de paquetes y entornos virtuales de Python mÃ¡s rÃ¡pida que pip.
(Si no desea usarlo, pip normal estÃ¡ bien).

```bash
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
git clone https://github.com/litagin02/Style-Bert-VITS2.git
cd Style-Bert-VITS2
uv venv venv
venv\Scripts\activate
uv pip install "torch<2.4" "torchaudio<2.4" --index-url https://download.pytorch.org/whl/cu118
uv pip install -r requirements.txt
python initialize.py  # Descarga modelos necesarios y el modelo TTS predeterminado
```
No olvide el Ãºltimo paso.

### SÃ­ntesis de voz

El editor de sÃ­ntesis de voz se inicia haciendo doble clic en `Editor.bat` o ejecutando `python server_editor.py --inbrowser` (use `--device cpu` para modo CPU). En la pantalla puede crear guiones cambiando la configuraciÃ³n para cada lÃ­nea, guardar, cargar y editar diccionarios.
Los modelos predeterminados se descargan al instalar, por lo que puede usarlos sin entrenar.

La parte del editor estÃ¡ separada en [otro repositorio](https://github.com/litagin02/Style-Bert-VITS2-Editor).

Para la WebUI de sÃ­ntesis de voz de versiones anteriores a 2.2, haga doble clic en `App.bat` o ejecute `python app.py`. TambiÃ©n puede abrir solo la pestaÃ±a de sÃ­ntesis con `Inference.bat`.

La estructura de archivos del modelo necesaria para la sÃ­ntesis es la siguiente (no necesita colocarla manualmente):
```
model_assets
â”œâ”€â”€ su_modelo
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ archivo_de_modelo1.safetensors
â”‚   â”œâ”€â”€ archivo_de_modelo2.safetensors
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ style_vectors.npy
â””â”€â”€ otro_modelo
    â”œâ”€â”€ ...
```
Para la inferencia se necesitan `config.json`, `*.safetensors` y `style_vectors.npy`. Si comparte modelos, comparta estos 3 archivos.

`style_vectors.npy` es necesario para controlar el estilo. Al entrenar, se genera un estilo promedio "Neutral" por defecto.
Si desea controlar el estilo con mÃ¡s detalle usando mÃºltiples estilos, consulte "GeneraciÃ³n de estilos" a continuaciÃ³n.

### Entrenamiento

- Para detalles de entrenamiento en CLI, consulte [aquÃ­](docs/CLI.md).
- Para entrenamiento en Paperspace consulte [aquÃ­](docs/paperspace.md), y en Colab [aquÃ­](http://colab.research.google.com/github/litagin02/Style-Bert-VITS2/blob/master/colab.ipynb).

Para el entrenamiento se necesitan varios archivos de audio de 2-14 segundos y sus datos de transcripciÃ³n.

- Si ya tiene archivos de audio divididos y datos de transcripciÃ³n (como un corpus existente), puede usarlos tal cual (corrigiendo el archivo de transcripciÃ³n si es necesario). Consulte "WebUI de Entrenamiento" abajo.
- Si no, y solo tiene archivos de audio (de cualquier longitud), se incluye una herramienta para crear un conjunto de datos listo para entrenar.

#### CreaciÃ³n de conjunto de datos

- Desde la pestaÃ±a "Crear Dataset" en la WebUI (`App.bat` o `python app.py`), puede dividir archivos de audio en longitudes adecuadas y transcribirlos automÃ¡ticamente. O use `Dataset.bat` para abrir esa pestaÃ±a sola.
- DespuÃ©s de seguir las instrucciones, puede entrenar directamente en la pestaÃ±a "Entrenamiento".

#### WebUI de Entrenamiento

- Siga las instrucciones en la pestaÃ±a "Entrenamiento" de la WebUI (`App.bat` o `python app.py`). O use `Train.bat`.

### GeneraciÃ³n de estilos

- Por defecto, se genera el estilo "Neutral" y estilos basados en las subcarpetas de la carpeta de entrenamiento.
- Esto es para quienes quieren crear estilos manualmente de otras formas.
- Desde la pestaÃ±a "Crear Estilos" de la WebUI (`App.bat` o `python app.py`), puede generar estilos usando archivos de audio. O use `StyleVectors.bat`.
- Es independiente del entrenamiento, por lo que puede hacerlo durante o despuÃ©s del entrenamiento tantas veces como quiera (el preprocesamiento debe haber terminado).

### API Server

Ejecute `python server_fastapi.py` en el entorno construido para iniciar el servidor API.
Verifique la especificaciÃ³n de la API en `/docs` despuÃ©s de iniciar.

- El lÃ­mite de caracteres de entrada es 100 por defecto. Esto se puede cambiar en `server.limit` de `config.yml`.
- Por defecto, CORS estÃ¡ permitido para todos los dominios. Cambie `server.origins` en `config.yml` para restringirlo a dominios confiables si es posible.

El servidor API del editor de sÃ­ntesis de voz se inicia con `python server_editor.py`. AÃºn no estÃ¡ muy desarrollado y solo implementa lo mÃ­nimo necesario para el [repositorio del editor](https://github.com/litagin02/Style-Bert-VITS2-Editor).

Para el despliegue web del editor, consulte [este Dockerfile](Dockerfile.deploy).

### FusiÃ³n (Merge)

Puede mezclar dos modelos en tÃ©rminos de "calidad de voz", "tono", "expresiÃ³n emocional" y "tempo" para crear un nuevo modelo, o "sumar la diferencia de otros dos modelos a un modelo", etc.
Desde la pestaÃ±a "FusiÃ³n" de la WebUI (`App.bat` o `python app.py`), puede seleccionar y fusionar modelos. O use `Merge.bat`.

### ConversiÃ³n ONNX

Desde la pestaÃ±a "ConversiÃ³n ONNX" o `ConvertONNX.bat`, puede convertir archivos safetensors entrenados a formato ONNX. Esto es Ãºtil si necesita archivos ONNX para librerÃ­as externas. Por ejemplo, en [Aivis Project](https://aivis-project.com/) puede usar [AIVM Generator](https://aivm-generator.aivis-project.com/) para crear modelos para Aivis Speech.

### EvaluaciÃ³n de naturalidad

Se proporciona un script usando [SpeechMOS](https://github.com/tarepan/SpeechMOS) como un indicador para elegir el mejor paso de entrenamiento:
```bash
python speech_mos.py -m <nombre_del_modelo>
```
Se mostrarÃ¡ la evaluaciÃ³n de naturalidad por paso y se guardarÃ¡n los resultados en `mos_results/mos_{nombre_modelo}.csv` y `.png`. Es solo una referencia que no considera acento o emociÃ³n, asÃ­ que lo mejor es escuchar y seleccionar.

## RelaciÃ³n con Bert-VITS2

BÃ¡sicamente es una ligera modificaciÃ³n de la estructura del modelo Bert-VITS2. Tanto el [modelo pre-entrenado antiguo](https://huggingface.co/litagin/Style-Bert-VITS2-1.0-base) como el [modelo pre-entrenado JP-Extra](https://huggingface.co/litagin/Style-Bert-VITS2-2.0-base-JP-Extra) son prÃ¡cticamente iguales a Bert-VITS2 v2.1 o JP-Extra (con pesos innecesarios eliminados y convertidos a safetensors).

Las diferencias especÃ­ficas son:

- FÃ¡cil de usar para quienes no saben Python o Git, como [EasyBertVits2](https://github.com/Zuntan03/EasyBertVits2).
- Cambio del modelo de incrustaciÃ³n de emociones (a [wespeaker-voxceleb-resnet34-LM](https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM) de 256 dimensiones, mÃ¡s una incrustaciÃ³n de identificaciÃ³n de hablante que de emociÃ³n).
- Se eliminÃ³ la cuantizaciÃ³n vectorial de la incrustaciÃ³n de emociones, dejÃ¡ndola como una capa totalmente conectada.
- Al crear el archivo de vectores de estilo `style_vectors.npy`, se puede generar voz especificando continuamente la intensidad del estilo.
- CreaciÃ³n de varias WebUI.
- Soporte para entrenamiento en bf16.
- Soporte para formato safetensors, uso predeterminado.
- Otras correcciones de errores menores y refactorizaciÃ³n.


## Referencias
AdemÃ¡s de la referencia original (abajo), utilicÃ© los siguientes repositorios:
- [Bert-VITS2](https://github.com/fishaudio/Bert-VITS2)
- [EasyBertVits2](https://github.com/Zuntan03/EasyBertVits2)

[El modelo pre-entrenado](https://huggingface.co/litagin/Style-Bert-VITS2-1.0-base) y la [versiÃ³n JP-Extra](https://huggingface.co/litagin/Style-Bert-VITS2-2.0-base-JP-Extra) son esencialmente tomados del [modelo base original de Bert-VITS2 v2.1](https://huggingface.co/Garydesu/bert-vits2_base_model-2.1) y [modelo pre-entrenado JP-Extra de Bert-VITS2](https://huggingface.co/Stardust-minus/Bert-VITS2-Japanese-Extra), asÃ­ que todos los crÃ©ditos van al autor original ([Fish Audio](https://github.com/fishaudio)):


AdemÃ¡s, el mÃ³dulo [text/user_dict/](text/user_dict) se basa en:
- [voicevox_engine](https://github.com/VOICEVOX/voicevox_engine)
y la licencia de este mÃ³dulo es LGPL v3.

## LICENCIA

Este repositorio estÃ¡ licenciado bajo la GNU Affero General Public License v3.0, igual que el repositorio original de Bert-VITS2. Para mÃ¡s detalles, vea [LICENSE](LICENSE).

AdemÃ¡s, el mÃ³dulo [text/user_dict/](text/user_dict) estÃ¡ licenciado bajo la GNU Lesser General Public License v3.0, heredado del repositorio original de VOICEVOX engine. Para mÃ¡s detalles, vea [LGPL_LICENSE](LGPL_LICENSE).



Abajo estÃ¡ el README.md original.
---

<div align="center">

<img alt="LOGO" src="https://cdn.jsdelivr.net/gh/fishaudio/fish-diffusion@main/images/logo_512x512.png" width="256" height="256" />

# Bert-VITS2

VITS2 Backbone with multilingual bert

For quick guide, please refer to `webui_preprocess.py`.

ç®€æ˜“æ•™ç¨‹è¯·å‚è§ `webui_preprocess.py`ã€‚

## è¯·æ³¨æ„ï¼Œæœ¬é¡¹ç›®æ ¸å¿ƒæ€è·¯æ¥æºäº[anyvoiceai/MassTTS](https://github.com/anyvoiceai/MassTTS) ä¸€ä¸ªéå¸¸å¥½çš„ttsé¡¹ç›®
## MassTTSçš„æ¼”ç¤ºdemoä¸º[aiç‰ˆå³°å“¥é”è¯„å³°å“¥æœ¬äºº,å¹¶æ‰¾å›äº†åœ¨é‡‘ä¸‰è§’å¤±è½çš„è…°å­](https://www.bilibili.com/video/BV1w24y1c7z9)

[//]: # (## æœ¬é¡¹ç›®ä¸[PlayVoice/vits_chinese]&#40;https://github.com/PlayVoice/vits_chinese&#41; æ²¡æœ‰ä»»ä½•å…³ç³»)

[//]: # ()
[//]: # (æœ¬ä»“åº“æ¥æºäºä¹‹å‰æœ‹å‹åˆ†äº«äº†aiå³°å“¥çš„è§†é¢‘ï¼Œæœ¬äººè¢«å…¶ä¸­çš„æ•ˆæœæƒŠè‰³ï¼Œåœ¨è‡ªå·±å°è¯•MassTTSä»¥åå‘ç°fsåœ¨éŸ³è´¨æ–¹é¢ä¸vitsæœ‰ä¸€å®šå·®è·ï¼Œå¹¶ä¸”trainingçš„pipelineæ¯”vitsæ›´å¤æ‚ï¼Œå› æ­¤æŒ‰ç…§å…¶æ€è·¯å°†bert)

## æˆç†Ÿçš„æ—…è¡Œè€…/å¼€æ‹“è€…/èˆ°é•¿/åšå£«/sensei/çŒé­”äºº/å–µå–µéœ²/Våº”å½“å‚é˜…ä»£ç è‡ªå·±å­¦ä¹ å¦‚ä½•è®­ç»ƒã€‚

### ä¸¥ç¦å°†æ­¤é¡¹ç›®ç”¨äºä¸€åˆ‡è¿åã€Šä¸­åäººæ°‘å…±å’Œå›½å®ªæ³•ã€‹ï¼Œã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹ï¼Œã€Šä¸­åäººæ°‘å…±å’Œå›½æ²»å®‰ç®¡ç†å¤„ç½šæ³•ã€‹å’Œã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹ä¹‹ç”¨é€”ã€‚
### ä¸¥ç¦ç”¨äºä»»ä½•æ”¿æ²»ç›¸å…³ç”¨é€”ã€‚
#### Video:https://www.bilibili.com/video/BV1hp4y1K78E
#### Demo:https://www.bilibili.com/video/BV1TF411k78w
#### QQ Groupï¼š815818430
## References
+ [anyvoiceai/MassTTS](https://github.com/anyvoiceai/MassTTS)
+ [jaywalnut310/vits](https://github.com/jaywalnut310/vits)
+ [p0p4k/vits2_pytorch](https://github.com/p0p4k/vits2_pytorch)
+ [svc-develop-team/so-vits-svc](https://github.com/svc-develop-team/so-vits-svc)
+ [PaddlePaddle/PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech)
+ [emotional-vits](https://github.com/innnky/emotional-vits)
+ [fish-speech](https://github.com/fishaudio/fish-speech)
+ [Bert-VITS2-UI](https://github.com/jiangyuxiaoxiao/Bert-VITS2-UI)
## æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…ä½œå‡ºçš„åŠªåŠ›
<a href="https://github.com/fishaudio/Bert-VITS2/graphs/contributors" target="_blank">
  <img src="https://contrib.rocks/image?repo=fishaudio/Bert-VITS2"/>
</a>

[//]: # (# æœ¬é¡¹ç›®æ‰€æœ‰ä»£ç å¼•ç”¨å‡å·²å†™æ˜ï¼Œbertéƒ¨åˆ†ä»£ç æ€è·¯æ¥æºäº[AIå³°å“¥]&#40;https://www.bilibili.com/video/BV1w24y1c7z9&#41;ï¼Œä¸[vits_chinese]&#40;https://github.com/PlayVoice/vits_chinese&#41;æ— ä»»ä½•å…³ç³»ã€‚æ¬¢è¿å„ä½æŸ¥é˜…ä»£ç ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯¹è¯¥å¼€å‘è€…çš„[ç¢°ç“·ï¼Œä¹ƒè‡³å¼€ç›’å¼€å‘è€…çš„è¡Œä¸º]&#40;https://www.bilibili.com/read/cv27101514/&#41;è¡¨ç¤ºå¼ºçƒˆè°´è´£ã€‚)
